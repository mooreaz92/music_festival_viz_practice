{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualizing web scraped music festival data in Tableau** \n",
    "\n",
    "*By Ryan Moore*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview and Objectives\n",
    "\n",
    "#### *Overview*\n",
    "\n",
    "The following is a personal project where publicly available tables containing music festival data are web scraped, combined, cleaned, and visualized in Tableau.\n",
    "\n",
    "#### *Objectives*\n",
    "\n",
    "This project will have five main objectives:\n",
    "\n",
    "1. Web scrape music festival data from Festival Alarm, a publicly available website that tracks music festival data (mainly in Europe)\n",
    "2. Combine the scraped data into a single table\n",
    "3. Perform a high-level analysis of the data and clean it as needed\n",
    "4. Export the data to Tableau and create visualizations for publication\n",
    "5. Document findings and analysis in a blog post\n",
    "\n",
    "#### *A note on web scraping*\n",
    "\n",
    "The Flatiron school data science boot camp teaches it's students to always check the terms and conditions of a website before web scraping. Festival Alarm does not have a terms and conditions page, but it does have a privacy policy page (however, it is in German, so this was translated using Google Translate).\n",
    "\n",
    "The privacy policy page does not explicitly state that web scraping is not allowed, but it does state that the website is not to be used for commercial purposes.I reached out to the website owners and waited a week for their response; none was given. Since this project is for educational purposes only and there is no explicit statement against web scraping, I decided to proceed with the project.\n",
    "\n",
    "If anyone from Festival Alarm reads this and would like me to take down this project, please reach out to me and I will do so immediately. You can email me at mooreaz92@gmail.com or message me on GitHub."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1: Web scrape the music festival data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website Overview**\n",
    "\n",
    "The target website looks like this:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"/home/mooreaz92/Personal_Projects/music_festival_viz_practice/images/website_screen_1.png\" alt=\"Website Screen 1\" width=\"50%\" style=\"border-radius: 10px;\">\n",
    "</div>\n",
    "\n",
    "The above example is for the year of 2023. In order to change years, you must click on the year in the top right corner of the screen. This will bring you to a page that looks like this:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"/home/mooreaz92/Personal_Projects/music_festival_viz_practice/images/website_screen_2.png\" alt=\"Website Screen 2\" width=\"30%\" style=\"border-radius: 10px;\">\n",
    "</div>\n",
    "\n",
    "From here, you can click on the year you want to view and a table like the first screenshot will pop up. The table contains the following fields (I have added what I believe each field to mean in italics):\n",
    "\n",
    "- Name \n",
    "  - *The name of the festival*\n",
    "- Date + [year being viewed] \n",
    "  - *The date of the festival*\n",
    "- Duration \n",
    "  - *The duration, in days, of the festival*\n",
    "- Where \n",
    "  - *Whether the festival is indoors or outdoors*\n",
    "- Category \n",
    "  - *The type of music played at the festival*\n",
    "- Genres \n",
    "  - *The genres of music played at the festival*\n",
    "- Country \n",
    "  - *The country the festival is located in*\n",
    "- Visitors \n",
    "  - *The number of visitors the festival had*\n",
    "- Price \n",
    "  - *The price of a ticket to the festival, in euros*\n",
    "- Links \n",
    "  - *Links to the festival's website, ticket site, and an option to add to your festival list*\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webscraping Strategy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To webscrape this data, I will use a combination of the `requests` and `BeautifulSoup` libraries. The `requests` library will be used to make the HTTP request to the website and the `BeautifulSoup` library will be used to parse the HTML and extract the data. High level, we will code a custom function that takes in a list of years and that does the following:\n",
    "\n",
    "1. Makes an HTTP request to the website\n",
    "2. Parses the HTML using `BeautifulSoup`\n",
    "3. Extracts the data from the HTML\n",
    "4. Returns the data in a list of dictionaries\n",
    "5. Combines the list of dictionaries into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing libraries\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### Doing a sample request to the website, and checking if the request was successful\n",
    "\n",
    "url = 'https://www.festival-alarm.com/us/Festivals-2023'\n",
    "response = requests.get(url)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful Soup Analysis**\n",
    "\n",
    "Before we can write the webscraping function it would be good if we could get a better understanding of the HTML structure of the website. To do this, we will use the `BeautifulSoup` library to parse the HTML and then use the `prettify()` method to print out the HTML in a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making a request to the website, and getting the HTML content\n",
    "\n",
    "url = 'https://www.festival-alarm.com/us/Festivals-2023'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "### Using prettify to make the HTML content more readable, and identifying the html elements that make up the table. (Commented out)\n",
    "\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking a look at the prettified text dump in a seperate text editor, it looks like this table uses the **table**, **tr**, and **td** html table elements. We can use this information when we code the webscraping function.\n",
    "\n",
    "We can use beautiful soup to pull the table element, and then use the .read_html pandas method to convert the table into a dataframe. This will be useful when we are testing our webscraping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 743 entries, 0 to 742\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Name         743 non-null    object \n",
      " 1   Date 2023    743 non-null    object \n",
      " 2   Duration     743 non-null    object \n",
      " 3   Where        743 non-null    object \n",
      " 4   Category     743 non-null    object \n",
      " 5   Genres       743 non-null    object \n",
      " 6   Country      743 non-null    object \n",
      " 7   Venue        743 non-null    object \n",
      " 8   Visitors     743 non-null    object \n",
      " 9   Price(s.f.)  473 non-null    object \n",
      " 10  Unnamed: 10  0 non-null      float64\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 64.0+ KB\n"
     ]
    }
   ],
   "source": [
    "### Parsing the HTML content using BeautifulSoup and converting it to a dataframe\n",
    "table = soup.find('table')\n",
    "\n",
    "### Converting the HTML table to a dataframe\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "### Printing the info of the dataframe\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the sample webscraping method is pulling everything from the site**\n",
    "\n",
    "Luckily for us, the website has content at the end of the table which shows a count of records. We can use this to check that our webscraping function is pulling all of the data from the website.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"/home/mooreaz92/Personal_Projects/music_festival_viz_practice/images/website_screen_3.png\" alt=\"Website Screen 3\" width=\"30%\" style=\"border-radius: 10px;\">\n",
    "</div>\n",
    "\n",
    "Looks like our webscraping function is pulling all of the data from the sample website page. Now we can move on to coding the webscraping function to bring in a bunch of years at once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2: Combine the scraped data into a single table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding the webscraping function**\n",
    "\n",
    "Now that we have a webscraping function that works for a single year, we can code a function that will pull in multiple years at once. \n",
    "\n",
    "The function will take in a range of years and will return a dataframe with all of the data from the years in the range. It would be useful to have the year be a column in the dataframe for each row since it currently only exists in the table header. We can do this by adding a column to the dataframe and filling it with the year that the data was pulled from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making a function that requests the HTML content of each url in range of years, appends the year, and then appends each together into a single dataframe\n",
    "\n",
    "def get_festival_data(start_year, end_year):\n",
    "    df_list = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f'https://www.festival-alarm.com/us/Festivals-{year}'\n",
    "        response = requests.get(url)\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        table = soup.find('table')\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        df['year'] = year\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing the final extract of the data**\n",
    "\n",
    "Now that we have a webscraping function that works for multiple years, we can use it to extract the data from the website. We will extract all the available data from the website, which is from 2014 to 2024 (future dates for upcoming festivals). We will perform high level checks on the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the data for the years 2014 to 2024\n",
    "\n",
    "df = get_festival_data(2014, 2024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 3: Perform a high-level analysis of the data and clean it as needed\n",
    "\n",
    "With the data extracted, we can now perform a high-level analysis of the data and clean it as needed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing the date column**\n",
    "\n",
    "From looking at the .info() of the dataframe, we can tell that the date range of the festival is in unique columns depending on the year the festival took place.\n",
    "\n",
    "For example, if a festival took place between July 14th, 2015 and July 18th, 2015, the row would be populated with 07/14-07/18 in the Date 2015 column and all other 'Date' columns would be blank. It would be better if we had a start_date and end_date column instead of a column for each year. We can do this by creating a function that takes in a row and returns the start and end date of the festival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making a function that extracts the date range\n",
    "\n",
    "def extract_date_range(row):\n",
    "    year = row['year']\n",
    "    date_columns = [col for col in row.index if col.startswith('Date')]\n",
    "    date_column = next((col for col in date_columns if str(year) in col), None)\n",
    "    \n",
    "    if date_column:\n",
    "        date_range = row[date_column]\n",
    "        start_date = date_range[:5] + \"/\" + str(year)\n",
    "        end_date = date_range[-5:] + \"/\" + str(year)\n",
    "        return start_date, end_date\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "### Applying the function to the dataframe\n",
    "\n",
    "df_date = df.copy()\n",
    "df_date['start_date'], df_date['end_date'] = zip(*df_date.apply(extract_date_range, axis=1))\n",
    "\n",
    "### Dropping the 'Date' columns with the exception of the newly created start_date and end_date columns\n",
    "\n",
    "df_date_cleaned = df_date.drop(columns=[col for col in df_date.columns if col.startswith('Date') and col not in ['start_date', 'end_date']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigating Nulls**\n",
    "\n",
    "We should also check for nulls in the dataframe. There is one column that contains only null values; this is likely a column that was added to the website after the data was scraped. We can drop this column.\n",
    "\n",
    "The remaining data looks good, so we can move on to the next step and create some visualizations to get a high level sense of the data and make sure it makes sense before exporting to Tableau."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot-encoding the genres column**\n",
    "\n",
    "The genres column contains a list of genres for each festival. We can one-hot-encode this column to make it easier to work with in Tableau.\n",
    "\n",
    "**Creating start and end date columns**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Date 2014', 'Duration', 'Where', 'Category', 'Genres',\n",
       "       'Country', 'Venue', 'Visitors', 'Price(s.f.)', 'Unnamed: 10', 'year',\n",
       "       'Date 2015', 'Date 2016', 'Date 2017', 'Date 2018', 'Date 2019',\n",
       "       'Date 2020', 'Date 2021', 'Date 2022', 'Date 2023', 'Date 2024'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
